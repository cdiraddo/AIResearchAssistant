# -*- coding: utf-8 -*-
"""Copy of AIResearchAss.ipynb

Automatically generated by Colaboratory.

"""
#dependencies and imports
!pip install "unstructured[local-inference]"
!apt-get install libmagic-dev
!apt-get install -y poppler-utils
!sudo apt install tesseract-ocr!

!pip install langchain
!pip install pinecone-client

!pip install openai

import os
import langchain

from langchain.llms import OpenAI
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import CharacterTextSplitter

from google.colab import drive
#linking drive and documents
drive.mount('/content/drive')

loader = DirectoryLoader('/content/drive/MyDrive/testdocsforaiass', glob='**/*.pdf')

documents = loader.load()

documents[:1]
#splitting texts
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

texts = text_splitter.split_documents(documents)

print(texts)
#test
#llm = OpenAI(openai_api_key="OPENAI_API_KEY")
# llm = OpenAI(temperature=0.9)
# llm.predict("What would be a good company name for a company that makes colorful socks?")

#vectorembeds
from langchain.embeddings.openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings()

model_name = 'text-embedding-ada-002'

embedding_model = OpenAIEmbeddings(
    model=model_name,
    openai_api_key="API KEY"
)
#index pinecone creation
!pip install tiktoken
import pinecone

from langchain.vectorstores import Pinecone

pinecone.init(api_key= "API KEY",
              environment="ENVI")

index_name = "firstindex"

docsearch = Pinecone.from_documents(texts, embedding_model, index_name=index_name)

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
#execute and tests
chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff")
query = "what is kongoya"
similar_docs = docsearch.similarity_search(query)
chain.run(input_documents=similar_docs, question=query)
